{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1b17d5c5-62b4-49c9-b3de-8cd7f15d7dee",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 2965/2965 [02:52<00:00, 17.22it/s]\n",
      "100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 3631/3631 [00:38<00:00, 94.78it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "e=0,loss=0.18898 f1=0.88228 ---------------------->best\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 2965/2965 [02:52<00:00, 17.18it/s]\n",
      "100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 3631/3631 [00:38<00:00, 93.11it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "e=1,loss=0.05452 f1=0.91560 ---------------------->best\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 2965/2965 [02:52<00:00, 17.19it/s]\n",
      "100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 3631/3631 [00:38<00:00, 93.72it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "e=2,loss=0.02792 f1=0.93931 ---------------------->best\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 2965/2965 [02:52<00:00, 17.19it/s]\n",
      "100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 3631/3631 [00:38<00:00, 94.51it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "e=3,loss=0.01653 f1=0.94525 ---------------------->best\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 2965/2965 [02:52<00:00, 17.19it/s]\n",
      "100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 3631/3631 [00:38<00:00, 94.00it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "e=4,loss=0.01130 f1=0.94888 ---------------------->best\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 2965/2965 [02:52<00:00, 17.19it/s]\n",
      "100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 3631/3631 [00:38<00:00, 94.19it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "e=5,loss=0.00847 f1=0.95078 ---------------------->best\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 2965/2965 [02:52<00:00, 17.18it/s]\n",
      "100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 3631/3631 [00:38<00:00, 95.22it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "e=6,loss=0.00654 f1=0.95539 ---------------------->best\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 2965/2965 [02:52<00:00, 17.19it/s]\n",
      "100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 3631/3631 [00:38<00:00, 93.90it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "e=7,loss=0.00531 f1=0.95550 ---------------------->best\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 2965/2965 [02:52<00:00, 17.20it/s]\n",
      "100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 3631/3631 [00:38<00:00, 94.52it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "e=8,loss=0.00452 f1=0.95645 ---------------------->best\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 2965/2965 [02:52<00:00, 17.19it/s]\n",
      "100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 3631/3631 [00:39<00:00, 92.38it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "e=9,loss=0.00382 f1=0.95545\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 2965/2965 [02:52<00:00, 17.19it/s]\n",
      "100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 3631/3631 [00:38<00:00, 93.62it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "e=10,loss=0.00345 f1=0.96024 ---------------------->best\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 2965/2965 [02:52<00:00, 17.20it/s]\n",
      "100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 3631/3631 [00:39<00:00, 92.77it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "e=11,loss=0.00297 f1=0.96177 ---------------------->best\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 2965/2965 [02:52<00:00, 17.19it/s]\n",
      "100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 3631/3631 [00:39<00:00, 91.48it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "e=12,loss=0.00285 f1=0.95844\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 2965/2965 [02:52<00:00, 17.20it/s]\n",
      "100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 3631/3631 [00:39<00:00, 92.33it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "e=13,loss=0.00267 f1=0.95541\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 2965/2965 [02:52<00:00, 17.21it/s]\n",
      "100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 3631/3631 [00:39<00:00, 91.76it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "e=14,loss=0.00252 f1=0.96193 ---------------------->best\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 2965/2965 [02:52<00:00, 17.20it/s]\n",
      "100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 3631/3631 [00:38<00:00, 93.16it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "e=15,loss=0.00216 f1=0.95992\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 2965/2965 [02:52<00:00, 17.19it/s]\n",
      "100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 3631/3631 [00:38<00:00, 94.34it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "e=16,loss=0.00212 f1=0.95946\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 2965/2965 [02:52<00:00, 17.20it/s]\n",
      "100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 3631/3631 [00:38<00:00, 93.22it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "e=17,loss=0.00214 f1=0.96313 ---------------------->best\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 2965/2965 [02:52<00:00, 17.20it/s]\n",
      "100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 3631/3631 [00:38<00:00, 93.70it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "e=18,loss=0.00174 f1=0.96261\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 2965/2965 [02:52<00:00, 17.20it/s]\n",
      "100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 3631/3631 [00:38<00:00, 93.25it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "e=19,loss=0.00188 f1=0.96399 ---------------------->best\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 2965/2965 [02:52<00:00, 17.19it/s]\n",
      "100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 3631/3631 [00:39<00:00, 92.52it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "e=20,loss=0.00178 f1=0.96547 ---------------------->best\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 2965/2965 [02:52<00:00, 17.18it/s]\n",
      "100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 3631/3631 [00:39<00:00, 93.08it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "e=21,loss=0.00166 f1=0.96104\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 2965/2965 [02:52<00:00, 17.19it/s]\n",
      "100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 3631/3631 [00:39<00:00, 91.61it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "e=22,loss=0.00147 f1=0.96238\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 2965/2965 [02:52<00:00, 17.18it/s]\n",
      "100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 3631/3631 [00:40<00:00, 89.57it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "e=23,loss=0.00166 f1=0.96451\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 2965/2965 [02:52<00:00, 17.18it/s]\n",
      "100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 3631/3631 [00:39<00:00, 92.87it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "e=24,loss=0.00145 f1=0.96486\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 2965/2965 [02:52<00:00, 17.19it/s]\n",
      "100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 3631/3631 [00:39<00:00, 91.93it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "e=25,loss=0.00131 f1=0.96264\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 2965/2965 [02:52<00:00, 17.19it/s]\n",
      "100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 3631/3631 [00:40<00:00, 90.63it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "e=26,loss=0.00151 f1=0.96428\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 2965/2965 [02:52<00:00, 17.20it/s]\n",
      "100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 3631/3631 [00:39<00:00, 91.90it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "e=27,loss=0.00136 f1=0.96594 ---------------------->best\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 2965/2965 [02:52<00:00, 17.19it/s]\n",
      "100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 3631/3631 [00:39<00:00, 91.57it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "e=28,loss=0.00129 f1=0.96654 ---------------------->best\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 2965/2965 [02:52<00:00, 17.19it/s]\n",
      "100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 3631/3631 [00:38<00:00, 93.39it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "e=29,loss=0.00139 f1=0.96346\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 2965/2965 [02:52<00:00, 17.18it/s]\n",
      "100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 3631/3631 [00:39<00:00, 92.15it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "e=30,loss=0.00124 f1=0.96639\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 2965/2965 [02:52<00:00, 17.20it/s]\n",
      "100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 3631/3631 [00:39<00:00, 91.78it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "e=31,loss=0.00122 f1=0.96784 ---------------------->best\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 2965/2965 [02:52<00:00, 17.19it/s]\n",
      "100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 3631/3631 [00:39<00:00, 92.57it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "e=32,loss=0.00112 f1=0.96758\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 2965/2965 [02:52<00:00, 17.19it/s]\n",
      "100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 3631/3631 [00:39<00:00, 91.32it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "e=33,loss=0.00122 f1=0.96769\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 2965/2965 [02:52<00:00, 17.20it/s]\n",
      "100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 3631/3631 [00:40<00:00, 89.35it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "e=34,loss=0.00111 f1=0.96736\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 2965/2965 [02:52<00:00, 17.20it/s]\n",
      "100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 3631/3631 [00:39<00:00, 91.21it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "e=35,loss=0.00117 f1=0.96596\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 2965/2965 [02:52<00:00, 17.18it/s]\n",
      "100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 3631/3631 [00:39<00:00, 91.19it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "e=36,loss=0.00105 f1=0.96746\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 2965/2965 [02:52<00:00, 17.17it/s]\n",
      "100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 3631/3631 [00:38<00:00, 93.40it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "e=37,loss=0.00108 f1=0.96477\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 2965/2965 [02:52<00:00, 17.19it/s]\n",
      "100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 3631/3631 [00:39<00:00, 90.96it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "e=38,loss=0.00107 f1=0.96623\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 2965/2965 [02:52<00:00, 17.19it/s]\n",
      "100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 3631/3631 [00:39<00:00, 91.18it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "e=39,loss=0.00102 f1=0.96794 ---------------------->best\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 2965/2965 [02:52<00:00, 17.19it/s]\n",
      "100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 3631/3631 [00:39<00:00, 91.96it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "e=40,loss=0.00101 f1=0.96668\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 2965/2965 [02:52<00:00, 17.20it/s]\n",
      "100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 3631/3631 [00:39<00:00, 91.44it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "e=41,loss=0.00110 f1=0.96825 ---------------------->best\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 2965/2965 [02:52<00:00, 17.18it/s]\n",
      "100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 3631/3631 [00:39<00:00, 92.44it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "e=42,loss=0.00092 f1=0.96834 ---------------------->best\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 2965/2965 [02:52<00:00, 17.18it/s]\n",
      "100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 3631/3631 [00:39<00:00, 92.63it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "e=43,loss=0.00092 f1=0.96818\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 2965/2965 [02:52<00:00, 17.19it/s]\n",
      "100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 3631/3631 [00:38<00:00, 93.17it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "e=44,loss=0.00095 f1=0.97094 ---------------------->best\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 2965/2965 [02:52<00:00, 17.20it/s]\n",
      "100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 3631/3631 [00:39<00:00, 91.15it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "e=45,loss=0.00091 f1=0.96901\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 2965/2965 [02:52<00:00, 17.20it/s]\n",
      "100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 3631/3631 [00:39<00:00, 92.46it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "e=46,loss=0.00086 f1=0.97092\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 2965/2965 [02:52<00:00, 17.19it/s]\n",
      "100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 3631/3631 [00:39<00:00, 92.80it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "e=47,loss=0.00087 f1=0.97090\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 2965/2965 [02:52<00:00, 17.20it/s]\n",
      "100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 3631/3631 [00:39<00:00, 91.89it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "e=48,loss=0.00091 f1=0.97045\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 2965/2965 [02:52<00:00, 17.19it/s]\n",
      "100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 3631/3631 [00:39<00:00, 91.27it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "e=49,loss=0.00083 f1=0.97128 ---------------------->best\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "Interrupted by user",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 368\u001b[0m\n\u001b[1;32m    365\u001b[0m tfidf_r \u001b[38;5;241m=\u001b[39m tfidf_alignment()\n\u001b[1;32m    367\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m(\u001b[38;5;28;01mTrue\u001b[39;00m):\n\u001b[0;32m--> 368\u001b[0m     sen \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43minput\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43m请输入:\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m    369\u001b[0m     \u001b[38;5;28mprint\u001b[39m(get_ner_result(model, tokenizer, sen, rule, tfidf_r,device,idx2tag))\n",
      "File \u001b[0;32m/data/anaconda3/envs/dailyuse/lib/python3.10/site-packages/ipykernel/kernelbase.py:1282\u001b[0m, in \u001b[0;36mKernel.raw_input\u001b[0;34m(self, prompt)\u001b[0m\n\u001b[1;32m   1280\u001b[0m     msg \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mraw_input was called, but this frontend does not support input requests.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   1281\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m StdinNotImplementedError(msg)\n\u001b[0;32m-> 1282\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_input_request\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1283\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mstr\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mprompt\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1284\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_parent_ident\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mshell\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1285\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_parent\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mshell\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1286\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpassword\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m   1287\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/data/anaconda3/envs/dailyuse/lib/python3.10/site-packages/ipykernel/kernelbase.py:1325\u001b[0m, in \u001b[0;36mKernel._input_request\u001b[0;34m(self, prompt, ident, parent, password)\u001b[0m\n\u001b[1;32m   1322\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mKeyboardInterrupt\u001b[39;00m:\n\u001b[1;32m   1323\u001b[0m     \u001b[38;5;66;03m# re-raise KeyboardInterrupt, to truncate traceback\u001b[39;00m\n\u001b[1;32m   1324\u001b[0m     msg \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mInterrupted by user\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m-> 1325\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mKeyboardInterrupt\u001b[39;00m(msg) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1326\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m:\n\u001b[1;32m   1327\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlog\u001b[38;5;241m.\u001b[39mwarning(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mInvalid Message:\u001b[39m\u001b[38;5;124m\"\u001b[39m, exc_info\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: Interrupted by user"
     ]
    }
   ],
   "source": [
    "#1-使用bert-base-chinese，无数据增强\n",
    "import random\n",
    "import torch\n",
    "from torch import nn\n",
    "import os\n",
    "import pickle\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from torch.utils.data import Dataset,DataLoader\n",
    "from transformers import BertModel,BertTokenizer\n",
    "from tqdm import tqdm\n",
    "from seqeval.metrics import f1_score\n",
    "import ahocorasick\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "cache_model = '1_bert_base_no_aug.pt'\n",
    "\n",
    "def get_data(path,max_len=None):\n",
    "    all_text,all_tag = [],[]\n",
    "    with open(path,'r',encoding='utf8') as f:\n",
    "        all_data = f.read().split('\\n')\n",
    "\n",
    "    sen,tag = [],[]\n",
    "    for data in all_data:\n",
    "        data = data.split(' ')\n",
    "        if(len(data)!=2):\n",
    "            if len(sen)>2:\n",
    "                all_text.append(sen)\n",
    "                all_tag.append(tag)\n",
    "            sen, tag = [], []\n",
    "            continue\n",
    "        te,ta = data\n",
    "        sen.append(te)\n",
    "        tag.append(ta)\n",
    "    if max_len is not None:\n",
    "        return all_text[:max_len], all_tag[:max_len]\n",
    "    return all_text,all_tag\n",
    "\n",
    "class rule_find:\n",
    "    def __init__(self):\n",
    "        self.idx2type = idx2type = [\"食物\", \"药品商\", \"治疗方法\", \"药品\",\"检查项目\",\"疾病\",\"疾病症状\",\"科目\"]\n",
    "        self.type2idx = type2idx = {\"食物\": 0, \"药品商\": 1, \"治疗方法\": 2, \"药品\": 3,\"检查项目\":4,\"疾病\":5,\"疾病症状\":6,\"科目\":7}\n",
    "        self.ahos = [ahocorasick.Automaton() for i in range(len(self.type2idx))]\n",
    "\n",
    "        for type in idx2type:\n",
    "            with open(os.path.join('data','ent_aug',f'{type}.txt'),encoding='utf-8') as f:\n",
    "                all_en = f.read().split('\\n')\n",
    "            for en in all_en:\n",
    "                en = en.split(' ')[0]\n",
    "                if len(en)>=2:\n",
    "                    self.ahos[type2idx[type]].add_word(en,en)\n",
    "        for i in range(len(self.ahos)):\n",
    "            self.ahos[i].make_automaton()\n",
    "\n",
    "    def find(self,sen):\n",
    "        rule_result = []\n",
    "        mp = {}\n",
    "        all_res = []\n",
    "        all_ty = []\n",
    "        for i in range(len(self.ahos)):\n",
    "            now = list(self.ahos[i].iter(sen))\n",
    "            all_res.extend(now)\n",
    "            for j in range(len(now)):\n",
    "                all_ty.append(self.idx2type[i])\n",
    "        if len(all_res) != 0:\n",
    "            all_res = sorted(all_res, key=lambda x: len(x[1]), reverse=True)\n",
    "            for i,res in enumerate(all_res):\n",
    "                be = res[0] - len(res[1]) + 1\n",
    "                ed = res[0]\n",
    "                if be in mp or ed in mp:\n",
    "                    continue\n",
    "                rule_result.append((be, ed, all_ty[i], res[1]))\n",
    "                for t in range(be, ed + 1):\n",
    "                    mp[t] = 1\n",
    "        return rule_result\n",
    "\n",
    "\n",
    "#找出tag(label)中的所有实体及其下表，为实体动态替换/随机掩码策略/实体动态拼接做准备\n",
    "def find_entities(tag):\n",
    "    result = []#[(2,3,'药品'),(7,10,'药品商')]\n",
    "    label_len = len(tag)\n",
    "    i = 0\n",
    "    while(i<label_len):\n",
    "        if(tag[i][0]=='B'):\n",
    "            type = tag[i].strip('B-')\n",
    "            j=i+1\n",
    "            while(j<label_len and tag[j][0]=='I'):\n",
    "                j += 1\n",
    "            result.append((i,j-1,type))\n",
    "            i=j\n",
    "        else:\n",
    "            i = i + 1\n",
    "    return result\n",
    "\n",
    "\n",
    "class tfidf_alignment():\n",
    "    def __init__(self):\n",
    "        eneities_path = os.path.join('data', 'ent_aug')\n",
    "        files = os.listdir(eneities_path)\n",
    "        files = [docu for docu in files if '.py' not in docu]\n",
    "\n",
    "        self.tag_2_embs = {}\n",
    "        self.tag_2_tfidf_model = {}\n",
    "        self.tag_2_entity = {}\n",
    "        for ty in files:\n",
    "            with open(os.path.join(eneities_path, ty), 'r', encoding='utf-8') as f:\n",
    "                entities = f.read().split('\\n')\n",
    "                entities = [ent for ent in entities if len(ent.split(' ')[0]) <= 15 and len(ent.split(' ')[0]) >= 1]\n",
    "                en_name = [ent.split(' ')[0] for ent in entities]\n",
    "                ty = ty.strip('.txt')\n",
    "                self.tag_2_entity[ty] = en_name\n",
    "                tfidf_model = TfidfVectorizer(analyzer=\"char\")\n",
    "                embs = tfidf_model.fit_transform(en_name).toarray()\n",
    "                self.tag_2_embs[ty] = embs\n",
    "                self.tag_2_tfidf_model[ty] = tfidf_model\n",
    "    def align(self,ent_list):\n",
    "        new_result = {}\n",
    "        for s,e,cls,ent in ent_list:\n",
    "            ent_emb = self.tag_2_tfidf_model[cls].transform([ent])\n",
    "            sim_score = cosine_similarity(ent_emb, self.tag_2_embs[cls])\n",
    "            max_idx = sim_score[0].argmax()\n",
    "            max_score = sim_score[0][max_idx]\n",
    "\n",
    "            if max_score >= 0.5:\n",
    "                new_result[cls]= self.tag_2_entity[cls][max_idx]\n",
    "        return new_result\n",
    "\n",
    "\n",
    "class Entity_Extend:\n",
    "    def __init__(self):\n",
    "        eneities_path = os.path.join('data','ent')\n",
    "        files = os.listdir(eneities_path)\n",
    "        files = [docu for docu in files if '.py' not in docu]\n",
    "\n",
    "        self.type2entity = {}\n",
    "        self.type2weight = {}\n",
    "        for type in files:\n",
    "            with open(os.path.join(eneities_path,type),'r',encoding='utf-8') as f:\n",
    "                entities = f.read().split('\\n')\n",
    "                en_name = [ent for ent in entities if len(ent.split(' ')[0])<=15 and len(ent.split(' ')[0])>=1]\n",
    "                en_weight = [1]*len(en_name)\n",
    "                type = type.strip('.txt')\n",
    "                self.type2entity[type] = en_name\n",
    "                self.type2weight[type] = en_weight\n",
    "    def no_work(self,te,tag,type):\n",
    "        return te,tag\n",
    "\n",
    "    # 1. 实体替换\n",
    "    def entity_replace(self,te,ta,type):\n",
    "        choice_ent = random.choices(self.type2entity[type],weights=self.type2weight[type],k=1)[0]\n",
    "        ta = [\"B-\"+type] + [\"I-\"+type]*(len(choice_ent)-1)\n",
    "        return list(choice_ent),ta\n",
    "\n",
    "    # 2. 实体掩盖\n",
    "    def entity_mask(self,te,ta,type):\n",
    "        if(len(te)<=3):\n",
    "            return te,ta\n",
    "        elif(len(te)<=5):\n",
    "            te.pop(random.randint(0,len(te)-1))\n",
    "        else:\n",
    "            te.pop(random.randint(0, len(te) - 1))\n",
    "            te.pop(random.randint(0, len(te) - 1))\n",
    "        ta = [\"B-\" + type] + [\"I-\" + type] * (len(te) - 1)\n",
    "        return te,ta\n",
    "\n",
    "    # 3. 实体拼接\n",
    "    def entity_union(self,te,ta,type):\n",
    "        words = ['和','与','以及']\n",
    "        wor = random.choice(words)\n",
    "        choice_ent = random.choices(self.type2entity[type],weights=self.type2weight[type],k=1)[0]\n",
    "        te = te+list(wor)+list(choice_ent)\n",
    "        ta = ta+['O']*len(wor)+[\"B-\"+type] + [\"I-\"+type]*(len(choice_ent)-1)\n",
    "        return te,ta\n",
    "    def entities_extend(self,text,tag,ents):\n",
    "        cho = [self.no_work,self.entity_union,self.entity_mask,self.entity_replace,self.no_work]\n",
    "        new_text = text.copy()\n",
    "        new_tag = tag.copy()\n",
    "        sign = 0\n",
    "        for ent in ents:\n",
    "            p = random.choice(cho)\n",
    "            te,ta = p(text[ent[0]:ent[1]+1],tag[ent[0]:ent[1]+1],ent[2])\n",
    "            new_text[ent[0] + sign:ent[1] + 1 + sign], new_tag[ent[0] + sign:ent[1] + 1 + sign] = te,ta\n",
    "            sign += len(te)-(ent[1]-ent[0]+1)\n",
    "\n",
    "        return new_text, new_tag\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "class Nerdataset(Dataset):\n",
    "    def __init__(self,all_text,all_label,tokenizer,max_len,tag2idx,is_dev=False,enhance_data=False):\n",
    "        self.all_text = all_text\n",
    "        self.all_label = all_label\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_len= max_len\n",
    "        self.tag2idx = tag2idx\n",
    "        self.is_dev = is_dev\n",
    "        self.entity_extend = Entity_Extend()\n",
    "        self.enhance_data = enhance_data\n",
    "    def __getitem__(self, x):\n",
    "        text, label = self.all_text[x], self.all_label[x]\n",
    "        if self.is_dev:\n",
    "            max_len = min(len(self.all_text[x])+2,500)\n",
    "        else:\n",
    "            # 几种策略\n",
    "            if self.enhance_data:\n",
    "                ents = find_entities(label)\n",
    "                text,label = self.entity_extend.entities_extend(text,label,ents)\n",
    "            max_len = self.max_len\n",
    "        text, label =text[:max_len - 2], label[:max_len - 2]\n",
    "\n",
    "        x_len = len(text)\n",
    "        assert len(text)==len(label)\n",
    "        text_idx = self.tokenizer.encode(text,add_special_token=True)\n",
    "        label_idx = [self.tag2idx['<PAD>']] + [self.tag2idx[i] for i in label] + [self.tag2idx['<PAD>']]\n",
    "\n",
    "        text_idx +=[0]*(max_len-len(text_idx))\n",
    "        label_idx +=[self.tag2idx['<PAD>']]*(max_len-len(label_idx))\n",
    "        return torch.tensor(text_idx),torch.tensor(label_idx),x_len\n",
    "    def __len__(self):\n",
    "        return len(self.all_text)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def build_tag2idx(all_tag):\n",
    "    tag2idx = {'<PAD>':0}\n",
    "    for sen in all_tag:\n",
    "        for tag in sen:\n",
    "            tag2idx[tag] = tag2idx.get(tag,len(tag2idx))\n",
    "    return tag2idx\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "class Bert_Model(nn.Module):\n",
    "    def __init__(self,model_name,hidden_size,tag_num,bi):\n",
    "        super().__init__()\n",
    "        self.bert = BertModel.from_pretrained(model_name)\n",
    "        self.gru = nn.RNN(input_size=768,hidden_size=hidden_size,num_layers=2,batch_first=True,bidirectional=bi)\n",
    "        if bi:\n",
    "            self.classifier = nn.Linear(hidden_size*2,tag_num)\n",
    "        else:\n",
    "            self.classifier = nn.Linear(hidden_size, tag_num)\n",
    "        self.loss_fn = nn.CrossEntropyLoss(ignore_index=0)\n",
    "    def forward(self,x,label=None):\n",
    "        bert_0,_ = self.bert(x,attention_mask=(x>0),return_dict=False)\n",
    "        gru_0,_ = self.gru(bert_0)\n",
    "        pre = self.classifier(gru_0)\n",
    "        if label is not None:\n",
    "            loss = self.loss_fn(pre.reshape(-1,pre.shape[-1]),label.reshape(-1))\n",
    "            return loss\n",
    "        else:\n",
    "            return torch.argmax(pre,dim=-1).squeeze(0)\n",
    "\n",
    "def merge(model_result_word,rule_result):\n",
    "    result = model_result_word+rule_result\n",
    "    result = sorted(result,key=lambda x:len(x[-1]),reverse=True)\n",
    "    check_result = []\n",
    "    mp = {}\n",
    "    for res in result:\n",
    "        if res[0] in mp or res[1] in mp:\n",
    "            continue\n",
    "        check_result.append(res)\n",
    "        for i in range(res[0],res[1]+1):\n",
    "            mp[i] = 1\n",
    "    return check_result\n",
    "\n",
    "def get_ner_result(model,tokenizer,sen,rule,tfidf_r,device,idx2tag):\n",
    "    sen_to = tokenizer.encode(sen, add_special_tokens=True, return_tensors='pt').to(device)\n",
    "\n",
    "    pre = model(sen_to).tolist()\n",
    "\n",
    "    pre_tag = [idx2tag[i] for i in pre[1:-1]]\n",
    "    model_result = find_entities(pre_tag)\n",
    "    model_result_word = []\n",
    "    for res in model_result:\n",
    "        word = sen[res[0]:res[1] + 1]\n",
    "        model_result_word.append((res[0], res[1], res[2], word))\n",
    "    rule_result = rule.find(sen)\n",
    "\n",
    "    merge_result = merge(model_result_word, rule_result)\n",
    "    # print('模型结果',model_result_word)\n",
    "    # print('规则结果',rule_result)\n",
    "    tfidf_result = tfidf_r.align(merge_result)\n",
    "    #print('整合结果', merge_result)\n",
    "    #print('tfidf对齐结果', tfidf_result)\n",
    "    return tfidf_result\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    all_text,all_label = get_data(os.path.join('data','ner_data_aug.txt'))\n",
    "    train_text, dev_text, train_label, dev_label = train_test_split(all_text, all_label, test_size = 0.02, random_state = 42)\n",
    "\n",
    "    #加载太慢了，预处理一下\n",
    "    if os.path.exists('tmp_data/tag2idx.npy'):\n",
    "        with open('tmp_data/tag2idx.npy','rb') as f:\n",
    "            tag2idx = pickle.load(f)\n",
    "    else:\n",
    "        tag2idx = build_tag2idx(all_label)\n",
    "        with open('tmp_data/tag2idx.npy','wb') as f:\n",
    "            pickle.dump(tag2idx,f)\n",
    "\n",
    "\n",
    "    idx2tag = list(tag2idx)\n",
    "\n",
    "    max_len = 50\n",
    "    epoch = 50\n",
    "    batch_size = 60\n",
    "    hidden_size = 128\n",
    "    bi = True\n",
    "    model_name='model/bert-base-chinese'#chinese-roberta-wwm-ext\n",
    "    tokenizer = BertTokenizer.from_pretrained(model_name)\n",
    "    lr =1e-5\n",
    "    is_train=True\n",
    "\n",
    "    device = torch.device('cuda:3') if torch.cuda.is_available()   else torch.device('cpu')\n",
    "\n",
    "    train_dataset = Nerdataset(train_text,train_label,tokenizer,max_len,tag2idx,enhance_data=False)\n",
    "    train_dataloader = DataLoader(train_dataset,batch_size=batch_size,shuffle=True)\n",
    "\n",
    "    dev_dataset = Nerdataset(dev_text, dev_label, tokenizer, max_len, tag2idx,is_dev=True)\n",
    "    dev_dataloader = DataLoader(dev_dataset, batch_size=1, shuffle=False)\n",
    "\n",
    "    model = Bert_Model(model_name,hidden_size,len(tag2idx),bi)\n",
    "    # if os.path.exists(f'model/best_roberta_gru_model_ent_aug.pt'):\n",
    "    #     model.load_state_dict(torch.load('model/best_roberta_gru_model_ent_aug.pt'))\n",
    "    model = model.to(device)\n",
    "    opt = torch.optim.Adam(model.parameters(),lr = lr)\n",
    "    bestf1 = -1\n",
    "    if is_train:\n",
    "        for e in range(epoch):\n",
    "            loss_sum = 0\n",
    "            ba = 0\n",
    "            for x,y,batch_len in tqdm(train_dataloader):\n",
    "                x = x.to(device)\n",
    "                y = y.to(device)\n",
    "                opt.zero_grad()\n",
    "                loss = model(x,y)\n",
    "                loss.backward()\n",
    "\n",
    "                opt.step()\n",
    "                loss_sum+=loss\n",
    "                ba += 1\n",
    "            all_pre = []\n",
    "            all_label = []\n",
    "            for x,y,batch_len in tqdm(dev_dataloader):\n",
    "                assert len(x)==len(y)\n",
    "                x = x.to(device)\n",
    "                pre = model(x)\n",
    "                pre = [idx2tag[i] for i in pre[1:batch_len+1]]\n",
    "                all_pre.append(pre)\n",
    "\n",
    "                label = [idx2tag[i] for i in y[0][1:batch_len+1]]\n",
    "                all_label.append(label)\n",
    "            f1 = f1_score(all_pre, all_label)\n",
    "            if f1>bestf1:\n",
    "                bestf1 = f1\n",
    "                print(f'e={e},loss={loss_sum / ba:.5f} f1={f1:.5f} ---------------------->best')\n",
    "                torch.save(model.state_dict(),f'model/{cache_model}.pt')\n",
    "            else:\n",
    "                print(f'e={e},loss={loss_sum/ba:.5f} f1={f1:.5f}')\n",
    "\n",
    "    rule = rule_find()\n",
    "    tfidf_r = tfidf_alignment()\n",
    "\n",
    "    while(True):\n",
    "        sen = input('请输入:')\n",
    "        print(get_ner_result(model, tokenizer, sen, rule, tfidf_r,device,idx2tag))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "79f97f00-b579-4b2c-9f69-57b87f3e65c6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 2965/2965 [02:52<00:00, 17.22it/s]\n",
      "100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 3631/3631 [00:39<00:00, 91.61it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "e=0,loss=0.21211 f1=0.86079 ---------------------->best\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 77%|████████████████████████████████████████████████████████████████████████████████████████▊                          | 2290/2965 [02:12<00:39, 17.29it/s]IOPub message rate exceeded.\n",
      "The Jupyter server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--ServerApp.iopub_msg_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "ServerApp.iopub_msg_rate_limit=1000.0 (msgs/sec)\n",
      "ServerApp.rate_limit_window=3.0 (secs)\n",
      "\n",
      "100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 2965/2965 [02:52<00:00, 17.18it/s]\n",
      "100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 3631/3631 [00:40<00:00, 90.59it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "e=28,loss=0.00126 f1=0.96347\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 2965/2965 [02:52<00:00, 17.19it/s]\n",
      "  3%|███▍                                                                                                                | 108/3631 [00:01<00:38, 90.93it/s]IOPub message rate exceeded.\n",
      "The Jupyter server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--ServerApp.iopub_msg_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "ServerApp.iopub_msg_rate_limit=1000.0 (msgs/sec)\n",
      "ServerApp.rate_limit_window=3.0 (secs)\n",
      "\n",
      "100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 3631/3631 [00:40<00:00, 90.10it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "e=34,loss=0.00104 f1=0.96097\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 2965/2965 [02:52<00:00, 17.18it/s]\n",
      "100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 3631/3631 [00:40<00:00, 89.37it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "e=35,loss=0.00094 f1=0.96765\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 2965/2965 [02:52<00:00, 17.16it/s]\n",
      "100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 3631/3631 [00:40<00:00, 89.26it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "e=36,loss=0.00099 f1=0.96687\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 2965/2965 [02:52<00:00, 17.17it/s]\n",
      "100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 3631/3631 [00:40<00:00, 88.96it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "e=37,loss=0.00096 f1=0.96710\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 2965/2965 [02:52<00:00, 17.16it/s]\n",
      "100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 3631/3631 [00:40<00:00, 89.63it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "e=38,loss=0.00090 f1=0.96466\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 2965/2965 [02:52<00:00, 17.19it/s]\n",
      "100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 3631/3631 [00:40<00:00, 88.80it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "e=39,loss=0.00093 f1=0.96773\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 2965/2965 [02:52<00:00, 17.16it/s]\n",
      "100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 3631/3631 [00:40<00:00, 89.47it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "e=40,loss=0.00098 f1=0.96680\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 2965/2965 [02:52<00:00, 17.18it/s]\n",
      "100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 3631/3631 [00:40<00:00, 88.74it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "e=41,loss=0.00089 f1=0.96501\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 2965/2965 [02:52<00:00, 17.18it/s]\n",
      "100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 3631/3631 [00:40<00:00, 89.27it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "e=42,loss=0.00087 f1=0.96805 ---------------------->best\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 2965/2965 [02:52<00:00, 17.17it/s]\n",
      "100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 3631/3631 [00:40<00:00, 90.13it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "e=43,loss=0.00104 f1=0.96524\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 2965/2965 [02:52<00:00, 17.19it/s]\n",
      "100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 3631/3631 [00:40<00:00, 89.86it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "e=44,loss=0.00080 f1=0.96820 ---------------------->best\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 2965/2965 [02:52<00:00, 17.17it/s]\n",
      "100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 3631/3631 [00:39<00:00, 91.08it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "e=45,loss=0.00088 f1=0.96624\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 2965/2965 [02:52<00:00, 17.18it/s]\n",
      "100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 3631/3631 [00:39<00:00, 91.27it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "e=46,loss=0.00083 f1=0.96423\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 2965/2965 [02:52<00:00, 17.18it/s]\n",
      "100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 3631/3631 [00:39<00:00, 91.64it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "e=47,loss=0.00078 f1=0.97054 ---------------------->best\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 50%|█████████████████████████████████████████████████████████▋                                                         | 1488/2965 [01:26<01:25, 17.29it/s]IOPub message rate exceeded.\n",
      "The Jupyter server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--ServerApp.iopub_msg_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "ServerApp.iopub_msg_rate_limit=1000.0 (msgs/sec)\n",
      "ServerApp.rate_limit_window=3.0 (secs)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#3-使用robert-base-chinese，无数据增强\n",
    "import random\n",
    "import torch\n",
    "from torch import nn\n",
    "import os\n",
    "import pickle\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from torch.utils.data import Dataset,DataLoader\n",
    "from transformers import BertModel,BertTokenizer\n",
    "from tqdm import tqdm\n",
    "from seqeval.metrics import f1_score\n",
    "import ahocorasick\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "cache_model = '4_robert_base_no_aug.pt'\n",
    "\n",
    "def get_data(path,max_len=None):\n",
    "    all_text,all_tag = [],[]\n",
    "    with open(path,'r',encoding='utf8') as f:\n",
    "        all_data = f.read().split('\\n')\n",
    "\n",
    "    sen,tag = [],[]\n",
    "    for data in all_data:\n",
    "        data = data.split(' ')\n",
    "        if(len(data)!=2):\n",
    "            if len(sen)>2:\n",
    "                all_text.append(sen)\n",
    "                all_tag.append(tag)\n",
    "            sen, tag = [], []\n",
    "            continue\n",
    "        te,ta = data\n",
    "        sen.append(te)\n",
    "        tag.append(ta)\n",
    "    if max_len is not None:\n",
    "        return all_text[:max_len], all_tag[:max_len]\n",
    "    return all_text,all_tag\n",
    "\n",
    "class rule_find:\n",
    "    def __init__(self):\n",
    "        self.idx2type = idx2type = [\"食物\", \"药品商\", \"治疗方法\", \"药品\",\"检查项目\",\"疾病\",\"疾病症状\",\"科目\"]\n",
    "        self.type2idx = type2idx = {\"食物\": 0, \"药品商\": 1, \"治疗方法\": 2, \"药品\": 3,\"检查项目\":4,\"疾病\":5,\"疾病症状\":6,\"科目\":7}\n",
    "        self.ahos = [ahocorasick.Automaton() for i in range(len(self.type2idx))]\n",
    "\n",
    "        for type in idx2type:\n",
    "            with open(os.path.join('data','ent_aug',f'{type}.txt'),encoding='utf-8') as f:\n",
    "                all_en = f.read().split('\\n')\n",
    "            for en in all_en:\n",
    "                en = en.split(' ')[0]\n",
    "                if len(en)>=2:\n",
    "                    self.ahos[type2idx[type]].add_word(en,en)\n",
    "        for i in range(len(self.ahos)):\n",
    "            self.ahos[i].make_automaton()\n",
    "\n",
    "    def find(self,sen):\n",
    "        rule_result = []\n",
    "        mp = {}\n",
    "        all_res = []\n",
    "        all_ty = []\n",
    "        for i in range(len(self.ahos)):\n",
    "            now = list(self.ahos[i].iter(sen))\n",
    "            all_res.extend(now)\n",
    "            for j in range(len(now)):\n",
    "                all_ty.append(self.idx2type[i])\n",
    "        if len(all_res) != 0:\n",
    "            all_res = sorted(all_res, key=lambda x: len(x[1]), reverse=True)\n",
    "            for i,res in enumerate(all_res):\n",
    "                be = res[0] - len(res[1]) + 1\n",
    "                ed = res[0]\n",
    "                if be in mp or ed in mp:\n",
    "                    continue\n",
    "                rule_result.append((be, ed, all_ty[i], res[1]))\n",
    "                for t in range(be, ed + 1):\n",
    "                    mp[t] = 1\n",
    "        return rule_result\n",
    "\n",
    "\n",
    "#找出tag(label)中的所有实体及其下表，为实体动态替换/随机掩码策略/实体动态拼接做准备\n",
    "def find_entities(tag):\n",
    "    result = []#[(2,3,'药品'),(7,10,'药品商')]\n",
    "    label_len = len(tag)\n",
    "    i = 0\n",
    "    while(i<label_len):\n",
    "        if(tag[i][0]=='B'):\n",
    "            type = tag[i].strip('B-')\n",
    "            j=i+1\n",
    "            while(j<label_len and tag[j][0]=='I'):\n",
    "                j += 1\n",
    "            result.append((i,j-1,type))\n",
    "            i=j\n",
    "        else:\n",
    "            i = i + 1\n",
    "    return result\n",
    "\n",
    "\n",
    "class tfidf_alignment():\n",
    "    def __init__(self):\n",
    "        eneities_path = os.path.join('data', 'ent_aug')\n",
    "        files = os.listdir(eneities_path)\n",
    "        files = [docu for docu in files if '.py' not in docu]\n",
    "\n",
    "        self.tag_2_embs = {}\n",
    "        self.tag_2_tfidf_model = {}\n",
    "        self.tag_2_entity = {}\n",
    "        for ty in files:\n",
    "            with open(os.path.join(eneities_path, ty), 'r', encoding='utf-8') as f:\n",
    "                entities = f.read().split('\\n')\n",
    "                entities = [ent for ent in entities if len(ent.split(' ')[0]) <= 15 and len(ent.split(' ')[0]) >= 1]\n",
    "                en_name = [ent.split(' ')[0] for ent in entities]\n",
    "                ty = ty.strip('.txt')\n",
    "                self.tag_2_entity[ty] = en_name\n",
    "                tfidf_model = TfidfVectorizer(analyzer=\"char\")\n",
    "                embs = tfidf_model.fit_transform(en_name).toarray()\n",
    "                self.tag_2_embs[ty] = embs\n",
    "                self.tag_2_tfidf_model[ty] = tfidf_model\n",
    "    def align(self,ent_list):\n",
    "        new_result = {}\n",
    "        for s,e,cls,ent in ent_list:\n",
    "            ent_emb = self.tag_2_tfidf_model[cls].transform([ent])\n",
    "            sim_score = cosine_similarity(ent_emb, self.tag_2_embs[cls])\n",
    "            max_idx = sim_score[0].argmax()\n",
    "            max_score = sim_score[0][max_idx]\n",
    "\n",
    "            if max_score >= 0.5:\n",
    "                new_result[cls]= self.tag_2_entity[cls][max_idx]\n",
    "        return new_result\n",
    "\n",
    "\n",
    "class Entity_Extend:\n",
    "    def __init__(self):\n",
    "        eneities_path = os.path.join('data','ent')\n",
    "        files = os.listdir(eneities_path)\n",
    "        files = [docu for docu in files if '.py' not in docu]\n",
    "\n",
    "        self.type2entity = {}\n",
    "        self.type2weight = {}\n",
    "        for type in files:\n",
    "            with open(os.path.join(eneities_path,type),'r',encoding='utf-8') as f:\n",
    "                entities = f.read().split('\\n')\n",
    "                en_name = [ent for ent in entities if len(ent.split(' ')[0])<=15 and len(ent.split(' ')[0])>=1]\n",
    "                en_weight = [1]*len(en_name)\n",
    "                type = type.strip('.txt')\n",
    "                self.type2entity[type] = en_name\n",
    "                self.type2weight[type] = en_weight\n",
    "    def no_work(self,te,tag,type):\n",
    "        return te,tag\n",
    "\n",
    "    # 1. 实体替换\n",
    "    def entity_replace(self,te,ta,type):\n",
    "        choice_ent = random.choices(self.type2entity[type],weights=self.type2weight[type],k=1)[0]\n",
    "        ta = [\"B-\"+type] + [\"I-\"+type]*(len(choice_ent)-1)\n",
    "        return list(choice_ent),ta\n",
    "\n",
    "    # 2. 实体掩盖\n",
    "    def entity_mask(self,te,ta,type):\n",
    "        if(len(te)<=3):\n",
    "            return te,ta\n",
    "        elif(len(te)<=5):\n",
    "            te.pop(random.randint(0,len(te)-1))\n",
    "        else:\n",
    "            te.pop(random.randint(0, len(te) - 1))\n",
    "            te.pop(random.randint(0, len(te) - 1))\n",
    "        ta = [\"B-\" + type] + [\"I-\" + type] * (len(te) - 1)\n",
    "        return te,ta\n",
    "\n",
    "    # 3. 实体拼接\n",
    "    def entity_union(self,te,ta,type):\n",
    "        words = ['和','与','以及']\n",
    "        wor = random.choice(words)\n",
    "        choice_ent = random.choices(self.type2entity[type],weights=self.type2weight[type],k=1)[0]\n",
    "        te = te+list(wor)+list(choice_ent)\n",
    "        ta = ta+['O']*len(wor)+[\"B-\"+type] + [\"I-\"+type]*(len(choice_ent)-1)\n",
    "        return te,ta\n",
    "    def entities_extend(self,text,tag,ents):\n",
    "        cho = [self.no_work,self.entity_union,self.entity_mask,self.entity_replace,self.no_work]\n",
    "        new_text = text.copy()\n",
    "        new_tag = tag.copy()\n",
    "        sign = 0\n",
    "        for ent in ents:\n",
    "            p = random.choice(cho)\n",
    "            te,ta = p(text[ent[0]:ent[1]+1],tag[ent[0]:ent[1]+1],ent[2])\n",
    "            new_text[ent[0] + sign:ent[1] + 1 + sign], new_tag[ent[0] + sign:ent[1] + 1 + sign] = te,ta\n",
    "            sign += len(te)-(ent[1]-ent[0]+1)\n",
    "\n",
    "        return new_text, new_tag\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "class Nerdataset(Dataset):\n",
    "    def __init__(self,all_text,all_label,tokenizer,max_len,tag2idx,is_dev=False,enhance_data=False):\n",
    "        self.all_text = all_text\n",
    "        self.all_label = all_label\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_len= max_len\n",
    "        self.tag2idx = tag2idx\n",
    "        self.is_dev = is_dev\n",
    "        self.entity_extend = Entity_Extend()\n",
    "        self.enhance_data = enhance_data\n",
    "    def __getitem__(self, x):\n",
    "        text, label = self.all_text[x], self.all_label[x]\n",
    "        if self.is_dev:\n",
    "            max_len = min(len(self.all_text[x])+2,500)\n",
    "        else:\n",
    "            # 几种策略\n",
    "            if self.enhance_data:\n",
    "                ents = find_entities(label)\n",
    "                text,label = self.entity_extend.entities_extend(text,label,ents)\n",
    "            max_len = self.max_len\n",
    "        text, label =text[:max_len - 2], label[:max_len - 2]\n",
    "\n",
    "        x_len = len(text)\n",
    "        assert len(text)==len(label)\n",
    "        text_idx = self.tokenizer.encode(text,add_special_token=True)\n",
    "        label_idx = [self.tag2idx['<PAD>']] + [self.tag2idx[i] for i in label] + [self.tag2idx['<PAD>']]\n",
    "\n",
    "        text_idx +=[0]*(max_len-len(text_idx))\n",
    "        label_idx +=[self.tag2idx['<PAD>']]*(max_len-len(label_idx))\n",
    "        return torch.tensor(text_idx),torch.tensor(label_idx),x_len\n",
    "    def __len__(self):\n",
    "        return len(self.all_text)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def build_tag2idx(all_tag):\n",
    "    tag2idx = {'<PAD>':0}\n",
    "    for sen in all_tag:\n",
    "        for tag in sen:\n",
    "            tag2idx[tag] = tag2idx.get(tag,len(tag2idx))\n",
    "    return tag2idx\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "class Bert_Model(nn.Module):\n",
    "    def __init__(self,model_name,hidden_size,tag_num,bi):\n",
    "        super().__init__()\n",
    "        self.bert = BertModel.from_pretrained(model_name)\n",
    "        self.gru = nn.RNN(input_size=768,hidden_size=hidden_size,num_layers=2,batch_first=True,bidirectional=bi)\n",
    "        if bi:\n",
    "            self.classifier = nn.Linear(hidden_size*2,tag_num)\n",
    "        else:\n",
    "            self.classifier = nn.Linear(hidden_size, tag_num)\n",
    "        self.loss_fn = nn.CrossEntropyLoss(ignore_index=0)\n",
    "    def forward(self,x,label=None):\n",
    "        bert_0,_ = self.bert(x,attention_mask=(x>0),return_dict=False)\n",
    "        gru_0,_ = self.gru(bert_0)\n",
    "        pre = self.classifier(gru_0)\n",
    "        if label is not None:\n",
    "            loss = self.loss_fn(pre.reshape(-1,pre.shape[-1]),label.reshape(-1))\n",
    "            return loss\n",
    "        else:\n",
    "            return torch.argmax(pre,dim=-1).squeeze(0)\n",
    "\n",
    "def merge(model_result_word,rule_result):\n",
    "    result = model_result_word+rule_result\n",
    "    result = sorted(result,key=lambda x:len(x[-1]),reverse=True)\n",
    "    check_result = []\n",
    "    mp = {}\n",
    "    for res in result:\n",
    "        if res[0] in mp or res[1] in mp:\n",
    "            continue\n",
    "        check_result.append(res)\n",
    "        for i in range(res[0],res[1]+1):\n",
    "            mp[i] = 1\n",
    "    return check_result\n",
    "\n",
    "def get_ner_result(model,tokenizer,sen,rule,tfidf_r,device,idx2tag):\n",
    "    sen_to = tokenizer.encode(sen, add_special_tokens=True, return_tensors='pt').to(device)\n",
    "\n",
    "    pre = model(sen_to).tolist()\n",
    "\n",
    "    pre_tag = [idx2tag[i] for i in pre[1:-1]]\n",
    "    model_result = find_entities(pre_tag)\n",
    "    model_result_word = []\n",
    "    for res in model_result:\n",
    "        word = sen[res[0]:res[1] + 1]\n",
    "        model_result_word.append((res[0], res[1], res[2], word))\n",
    "    rule_result = rule.find(sen)\n",
    "\n",
    "    merge_result = merge(model_result_word, rule_result)\n",
    "    # print('模型结果',model_result_word)\n",
    "    # print('规则结果',rule_result)\n",
    "    tfidf_result = tfidf_r.align(merge_result)\n",
    "    #print('整合结果', merge_result)\n",
    "    #print('tfidf对齐结果', tfidf_result)\n",
    "    return tfidf_result\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    all_text,all_label = get_data(os.path.join('data','ner_data_aug.txt'))\n",
    "    train_text, dev_text, train_label, dev_label = train_test_split(all_text, all_label, test_size = 0.02, random_state = 42)\n",
    "\n",
    "    #加载太慢了，预处理一下\n",
    "    if os.path.exists('tmp_data/tag2idx.npy'):\n",
    "        with open('tmp_data/tag2idx.npy','rb') as f:\n",
    "            tag2idx = pickle.load(f)\n",
    "    else:\n",
    "        tag2idx = build_tag2idx(all_label)\n",
    "        with open('tmp_data/tag2idx.npy','wb') as f:\n",
    "            pickle.dump(tag2idx,f)\n",
    "\n",
    "\n",
    "    idx2tag = list(tag2idx)\n",
    "\n",
    "    max_len = 50\n",
    "    epoch = 50\n",
    "    batch_size = 60\n",
    "    hidden_size = 128\n",
    "    bi = True\n",
    "    model_name='model/chinese-roberta-wwm-ext'#chinese-roberta-wwm-ext\n",
    "    tokenizer = BertTokenizer.from_pretrained(model_name)\n",
    "    lr =1e-5\n",
    "    is_train=True\n",
    "\n",
    "    device = torch.device('cuda:3') if torch.cuda.is_available()   else torch.device('cpu')\n",
    "\n",
    "    train_dataset = Nerdataset(train_text,train_label,tokenizer,max_len,tag2idx,enhance_data=False)\n",
    "    train_dataloader = DataLoader(train_dataset,batch_size=batch_size,shuffle=True)\n",
    "\n",
    "    dev_dataset = Nerdataset(dev_text, dev_label, tokenizer, max_len, tag2idx,is_dev=True)\n",
    "    dev_dataloader = DataLoader(dev_dataset, batch_size=1, shuffle=False)\n",
    "\n",
    "    model = Bert_Model(model_name,hidden_size,len(tag2idx),bi)\n",
    "    # if os.path.exists(f'model/best_roberta_gru_model_ent_aug.pt'):\n",
    "    #     model.load_state_dict(torch.load('model/best_roberta_gru_model_ent_aug.pt'))\n",
    "    model = model.to(device)\n",
    "    opt = torch.optim.Adam(model.parameters(),lr = lr)\n",
    "    bestf1 = -1\n",
    "    if is_train:\n",
    "        for e in range(epoch):\n",
    "            loss_sum = 0\n",
    "            ba = 0\n",
    "            for x,y,batch_len in tqdm(train_dataloader):\n",
    "                x = x.to(device)\n",
    "                y = y.to(device)\n",
    "                opt.zero_grad()\n",
    "                loss = model(x,y)\n",
    "                loss.backward()\n",
    "\n",
    "                opt.step()\n",
    "                loss_sum+=loss\n",
    "                ba += 1\n",
    "            all_pre = []\n",
    "            all_label = []\n",
    "            for x,y,batch_len in tqdm(dev_dataloader):\n",
    "                assert len(x)==len(y)\n",
    "                x = x.to(device)\n",
    "                pre = model(x)\n",
    "                pre = [idx2tag[i] for i in pre[1:batch_len+1]]\n",
    "                all_pre.append(pre)\n",
    "\n",
    "                label = [idx2tag[i] for i in y[0][1:batch_len+1]]\n",
    "                all_label.append(label)\n",
    "            f1 = f1_score(all_pre, all_label)\n",
    "            if f1>bestf1:\n",
    "                bestf1 = f1\n",
    "                print(f'e={e},loss={loss_sum / ba:.5f} f1={f1:.5f} ---------------------->best')\n",
    "                torch.save(model.state_dict(),f'model/{cache_model}.pt')\n",
    "            else:\n",
    "                print(f'e={e},loss={loss_sum/ba:.5f} f1={f1:.5f}')\n",
    "\n",
    "    rule = rule_find()\n",
    "    tfidf_r = tfidf_alignment()\n",
    "\n",
    "    # while(True):\n",
    "    #     sen = input('请输入:')\n",
    "    #     print(get_ner_result(model, tokenizer, sen, rule, tfidf_r,device,idx2tag))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "311f36fb-90d1-4b49-bf79-76909a0fafa4",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
